{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc657caa-b56e-46c7-bb13-d66e4caa38d1",
   "metadata": {},
   "source": [
    "# Building a Neural Network\n",
    "---\n",
    "We will start by going over some important concepts:\n",
    "## Linear Regression\n",
    "This is a method of modelling relationships between 2+ independant variables and a dependant variable. Each independant variable has a weirght associated with it which determines how much it impacts on the final result, and this is multiplied by the value of the variable itself. A bias is also applied, which helps achieve a result if the unbiassed value is zero, or needs to be a certain size.\n",
    "## Dot Product\n",
    "Lovely further maths. The dot product can be used to determine how similar two vectors are. It is the sum of the products of each item in each vector (that sounds horrible, so I shall give an example). Take the two vectors 4i+2j and 3i+j. Their dot product is 14, which is 4x3 + 1x2. An example of how it determines similarity is with perpendicular vectors. The vectors i and j have a dot product of 0, as they are perpendicular. This shows they are completely unlike one another.\n",
    "Beneath is an example of how the dot product of two vectors (represented as arrays) can be found using raw python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "974927d3-add5-456d-8a37-285d4dda4fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dot product is 2.1672\n"
     ]
    }
   ],
   "source": [
    "# Define vectors\n",
    "input_vector = [1.72, 1.23]\n",
    "weight_1 = [1.26, 0]\n",
    "\n",
    "# Calculate dot product\n",
    "i1_prod = input_vector[0] * weight_1[0]\n",
    "i2_prod = input_vector[1] * weight_1[1]\n",
    "dp_iv_w1 = i1_prod + i2_prod\n",
    "\n",
    "# Output the dot product\n",
    "print(f\"The dot product is {dp_iv_w1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7168a769-1cbf-42ff-8419-ea283ae94032",
   "metadata": {},
   "source": [
    "### Doing it a lot quicker...\n",
    "As much as it is useful to see how this works in raw python, it is a lot quicker to use **numpy** to do it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9706f65f-7d9f-45ac-ab9e-6fd35badd631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dot product from numpy is 2.1672\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dp_iv_w1 = np.dot(input_vector, weight_1)\n",
    "print(f\"The dot product from numpy is {dp_iv_w1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16df4be-23c6-43d0-964d-b25525bc3292",
   "metadata": {},
   "source": [
    "### Comparing values\n",
    "As I mentioned earlier, the dot product can be used to measure how similar two vectors are. Lets find the dot product of another vector and the input vector and compare them, to see which is more alike to the input vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49221eed-4498-4a0d-8629-7a8741fc51f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dot product with weight 2 is 4.1259999999999994\n"
     ]
    }
   ],
   "source": [
    "# Define new vector\n",
    "weight_2 = [2.17, 0.32]\n",
    "dp_iv_w2 = np.dot(input_vector, weight_2)\n",
    "print(f\"The dot product with weight 2 is {dp_iv_w2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edf1800-1cf6-4dd0-a6c4-c8360144fb28",
   "metadata": {},
   "source": [
    "We can see that 2.2 < 4.1, and therefore can determine that weight 2 is more alike the input vector than weight 1 (building on the idea from earlier that 0 represents 2 completely unlike vectors). This can clearly be seen when we plot the vectors on a graph:\n",
    "![graph of the vectors](https://realpython.com/cdn-cgi/image/width=1157,format=auto/https://files.realpython.com/media/three_vectors_2d.9d220456ff49.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e5138-386d-45ce-af40-0851e66df410",
   "metadata": {},
   "source": [
    "## Linear vs Nonlinear Functions\n",
    "More fun maths! The products and sums we have done so far have all been **linear**, meaning they can be traced back to their original value via some process. This can be useful, but after a while makes adding complexity to a network difficult. Say you have 10 layers of linear functions, each of which adds a value to the result of the previous. Although each layer is useful, the role of all 10 linear functions can be done by a single linear layer (adding the sum of all the values at once). This shows that when using multiple linear functions, the same result can always be achieved with fewer layers. To get around this, we can use **nonlinear functions** (also known as **activation functions**),specifically **the sigmoid function**. This resricts all outputs to be between 0 and 1, and is as follows:   \n",
    "\n",
    "$ S(x) = \\frac{1}{1+e^{-x}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde10497-665d-45f5-8ada-9f6ab87c8996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the sigmoid function, 2.5 becomes 0.9241418199787566\n"
     ]
    }
   ],
   "source": [
    "# Example use of the sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-1 * x))\n",
    "\n",
    "print(f\"Using the sigmoid function, 2.5 becomes {sigmoid(2.5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcc92b6-23a4-46cb-b976-6d560ecb63a4",
   "metadata": {},
   "source": [
    "As you can see, the number has been mapped to be between 0 and 1! You may be wondering what the output value actually represents, and this is based on a Bernouilli distribution, which in itself is a special case of the Binomial distribution where n=1 (so the only outcomes are P(X=x)=1 or =0). Essentially, large positive inputs are represented closer to 1 and smaller negative inputs (further from 0) are represented towards 0.   \n",
    "In our case, we will round values to either 0 or 1, to limit the outputs of our network to 0 and 1. That is also why this will be used in our last layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78259fe-ec97-4be3-b3f2-4120e9a463d1",
   "metadata": {},
   "source": [
    "## Back to the Neural network...\n",
    "Let us first define what we want our AI to do. We will aim to make predictions that can only have the outcome 0 or 1 (a classification problem) and determine which one our input variable maps to. To keep things simple, we will only use numbers, but this is a similar concept to all those cool programs you see that can predict the contents of an image, just a lot more basic. We will simply aim to get a result from 2 possible inputs, as shown here:\n",
    "| Input | Output we want |\n",
    "| ------| ---------------|\n",
    "| 1.66, 1.56 | 1 |\n",
    "| 2, 1.5 | 0 |\n",
    "\n",
    "### Layers\n",
    "We will use 2 layers in our network. The first will find the dot product of the input with our weight vectors, and then the second will run this result through the sigmoid function and map our result to either 0 or 1. As explained earlier, we only need 1 linear function layer, so having 1 linear and 1 nonlinear works well.\n",
    "### Predicting\n",
    "Lets tie this all together. Below is an implimentation which just takes our input vector, a fixed weight, and a bias. This currently uses no adjustment of bias or self-correction, just an initial guess to start us off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e7ee7dd-1964-4ce5-a639-a58b83ddd081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of our first prediction is 0.7985731042296965\n"
     ]
    }
   ],
   "source": [
    "# Define our vectors within numpy arrays\n",
    "input_vector = np.array([1.66, 1.56])\n",
    "weights_1 = np.array([1.45, -0.66])\n",
    "bias = np.array([0.0])\n",
    "\n",
    "# The neural network layers\n",
    "def predict(inp, weights, bias):\n",
    "    layer_1 = np.dot(input_vector, weights_1) + bias\n",
    "    layer_2 = sigmoid(layer_1)\n",
    "    return layer_2\n",
    "\n",
    "# Make a prediction using our vectors\n",
    "prediction = predict(input_vector, weights_1, bias)\n",
    "\n",
    "print(f\"The result of our first prediction is {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5b2b7f-cb18-4148-91dc-39764cd5b20b",
   "metadata": {},
   "source": [
    "As our result is greater than 0.5, this maps to 1, and thus the network was successful! Yippee! Now let's test it with our other input, (2, 1.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e34b53cf-8e95-4fd6-8b6b-3b15cfa8ad9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of our second prediction is 0.8710191478537642\n"
     ]
    }
   ],
   "source": [
    "# Redefine our input vector\n",
    "input_vector = np.array([2, 1.5])\n",
    "\n",
    "# Make the new prediction\n",
    "prediction = predict(input_vector, weights_1, bias)\n",
    "\n",
    "print(f\"The result of our second prediction is {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69186154-ea34-452c-b9c8-743492e27e0a",
   "metadata": {},
   "source": [
    "0.87 also rounds to 1, which is not the result we wanted (oh dear). Lets look at how to our model can improve itself and correct this mistake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c344c7f1-feec-4f2e-9371-07d7cfd34548",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "There are a few stages involved in adjusting our weights to fix the model.\n",
    "1. Calculate prediction error\n",
    "2. Adjust weights to reduce error\n",
    "3. More stuff that I need to ADD HERE\n",
    "\n",
    "### Prediction error\n",
    "The function we use to find the prediction error is called the **cost function**, and we will use **mean squared error (MSE)** to calculate this. MSE works by squaring the difference between the target and the predicted value, which ensures it is always a positive value. Squaring the value also means that larger errors create larger values and smaller errors have smaller values. Lets find the MSE for our incorrect prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d66e6fa-9806-47ec-abd9-f5c3ababd1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction of 0.8710191478537642 has a MSE of 0.7586743559278976\n"
     ]
    }
   ],
   "source": [
    "# Define our target and calculate MSE\n",
    "target = 0\n",
    "mse = np.square(prediction - target)\n",
    "print(f\"Prediction of {prediction[0]} has a MSE of {mse[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0f9913-1e92-46dd-88c7-1d08e4c00918",
   "metadata": {},
   "source": [
    "So we have a value of 0.76 for our prediction error, cool!  \n",
    "Now we want to figure out how to reduce this (warning, this is where the fun maths comes in).  \n",
    "If we treat (prediction - target) as a function, np.square produces a **quadratic curve** where the y-axis represents our error value. In order to minimise our y-value, we can differentiate! We use this inside a thing called the **gradient descent algorithm**, which works by differentiating the function and comparing the gradient at your error value to 0. If it is positive, the prediction was too high and should be reduced, and vice versa for a negative value. Lets try this out on our value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5487e169-e251-4f4f-b6ea-11c45f12df1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derivative at our point is 1.7420382957075284\n"
     ]
    }
   ],
   "source": [
    "# Differentiate our value (x^2 becomes 2x)\n",
    "derivative = 2 * (prediction - target)\n",
    "print(f\"The derivative at our point is {derivative[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abf8268-dba1-4815-8264-00f59f416ee2",
   "metadata": {},
   "source": [
    "Great! As the derivative is positive, we know we must reduce it.  \n",
    "Inside our gradient descent algorithm, we use this value for the derivative (gradient) to change our weights and recalculate a new prediction. There is one problem though. If we have large values for our derivative, we will just keep fluctuating either side of the minimum value (like a pendulum). One time we will be over and move back under, and the next we will move back over. To reduce this from happening, we use **the alpha parameter**, or **learning rate**. This is a fraction which the derivative is multiplied by in order to reduce the incriments by which the weights are changed. Finding the ideal learning rate is a matter of experimenting with values! We can impliment this later, but for now lets just see how using the derivative itself impacts our error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d16f53a3-528d-4471-8324-1dcf174e4c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For prediction 1: target = 1, prediction = 0.014962479449257616, error = 0.9702989168927542\n",
      "For prediction 2: target = 0, prediction = 0.014962479449257616, error = 0.00022387579126945649\n"
     ]
    }
   ],
   "source": [
    "# Reduce weights by the value of the derivative\n",
    "weights_1 -= derivative\n",
    "\n",
    "# Create function to calculate MSE\n",
    "def calc_mse(prediction, target):\n",
    "    return (prediction - target) ** 2\n",
    "\n",
    "# Re-predict for both values\n",
    "prediction_1 = predict([1.66, 1.56], weights_1, bias)\n",
    "prediction_2 = predict([2, 1.5], weights_1, bias)\n",
    "\n",
    "# Calculate MSE for both values\n",
    "mse_1 = calc_mse(prediction_1, 1)\n",
    "mse_2 = calc_mse(prediction_2, 0)\n",
    "\n",
    "print(f\"For prediction 1: target = 1, prediction = {prediction_1[0]}, error = {mse_1[0]}\")\n",
    "print(f\"For prediction 2: target = 0, prediction = {prediction_2[0]}, error = {mse_2[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faf994f-df2c-4659-ac5b-3857613e05ae",
   "metadata": {},
   "source": [
    "Nice! Our prediction 2 is now correct and has a far smaller error! Slight problem though, our first prediction is now wrong :(  \n",
    "This is not neccessarily a bad thing. If our model always predicted correctly, it could be **overfitted**, meaning it is too well adapted to the training data and simply remembers the numbers rather than learning to recognise them. In this case though, we can still train our model more to make it better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744ad644-bd00-409b-81b7-c16c40998d0d",
   "metadata": {},
   "source": [
    "## Backprobagation\n",
    "This is another scary word, but is essentially just working back through the network.  \n",
    "We use backprobagation to find better values for our weights and biases, using similar principles to what we did earlier with differentiation. This time we will use the [chain rule](https://www.youtube.com/watch?v=BIu0m2DObAA) to find the derivative of not just the function $x^2$, but our whole function, $(prediction - target)^2$. The chain rule is needed as we are prerfoming a function (squaring) on another function (prediction - target). Another way of looking at this is to work backwards through our two layers (known as a **backwards pass**). We have the first layer (dot product) and second layer (sigmoid), so working backwards gives us the process of error --> sigmoid --> dot product --> bias. Using the chain rule, where we want to differentiate the error value with respect to the weights, this gives us the following formula:  \n",
    "$\\frac{d(error)}{d(bias)} = \\frac{d(error)}{d(sigmoid)} \\times \\frac{d(sigmoid)}{d(dot product)} \\times \\frac{d(dot product)}{d(bias)}$  \n",
    "Now let's actually do this with some code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "910450d7-61cf-4f34-b59b-eaa3e6ce9f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set prediction variable (we are working with our prediction for option 2)\n",
    "prediction = prediction_2\n",
    "\n",
    "# Function to find the derivative of the sigmoid function (with value x)\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "# Calculate partial derivatives to work towards derivative of error with respect to weights\n",
    "derror_dsigmoid = 2*(prediction-target)\n",
    "layer_1 = np.dot(input_vector, weights_1) + bias\n",
    "dsigmoid_ddotprod = sigmoid_deriv(layer_1)\n",
    "ddotprod_dbias = 1  # Is an independant variable, so differentiates to 1\n",
    "derror_dbias = derror_dsigmoid * dsigmoid_ddotprod * ddotprod_dbias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8859ed9f-7ea9-465f-ab01-edc2ba41f3d7",
   "metadata": {},
   "source": [
    "## Making the network (again...)\n",
    "Now we (hopefully) know what is going on, its time to make a class for our network!  \n",
    "We will define our weights and bias as random and then use the **backprobagation algorithm** to improve their values through training. The learning rate is passed as a parameter.  \n",
    "Also, because python OOP isn't that great, we will denote private functions with an underscore (e.g. _foo())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10732b67-2b2e-4b2e-9477-83d500eaaf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    # Constructor method\n",
    "    def __init__(self, learning_rate):\n",
    "        self.weights = np.array([np.random.randn(), np.random.randn()])\n",
    "        self.bias = np.random.randn()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    # Sigmoid function (layer 2)\n",
    "    def _sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    # Derivative of sigmoid (backprogagation)\n",
    "    def _sigmoid_deriv(self, x):\n",
    "        return self._sigmoid(x) * (1 - self._sigmoid(x))\n",
    "\n",
    "    # Make a prediction using the network\n",
    "    def predict(self, input_vector):\n",
    "        layer_1 = np.dot(input_vector, self.weights) + self.bias\n",
    "        layer_2 = self._sigmoid(layer_1)\n",
    "        prediction = layer_2\n",
    "        return prediction\n",
    "\n",
    "    # Find the gradients of the error functions (backprobagation)\n",
    "    def _compute_gradients(self, input_vector, target):\n",
    "        layer_1 = np.dot(input_vector, self.weights) + self.bias\n",
    "        layer_2 = self._sigmoid(layer_1)\n",
    "        prediction = layer_2\n",
    "\n",
    "        derror_dprediction = 2*(prediction - target)\n",
    "        dprediction_dlayer1 = self._sigmoid_deriv(layer_1)\n",
    "        dlayer1_dbias = 1\n",
    "        dlayer1_dweights = (0 * self.weights) + (1 * input_vector)\n",
    "\n",
    "        derror_dbias = derror_dprediction * dprediction_dlayer1 * dlayer1_dbias\n",
    "        derror_dweights = derror_dprediction * dprediction_dlayer1 * dlayer1_dweights\n",
    "        return derror_bias, derror_dweights\n",
    "\n",
    "    # Change weight and bias parameters (result of backprobagation)\n",
    "    def _update_paramters(self, derror_dbias, derror_dweights):\n",
    "        self.bias = self.bias - (derror_dbias * self.learning_rate)\n",
    "        self.weights = self.weights - (derror_dweights * self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f7b679-74b2-4463-b1c3-1f6f967fe76b",
   "metadata": {},
   "source": [
    "Now we have our class, lets give it a go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58776658-3283-47de-ad6b-d1b705098b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.01955019514289159)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "neural_network = NeuralNetwork(learning_rate)\n",
    "neural_network.predict(input_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
